üöÄ LOCAL AI ARCHITECTURE: MCP vs. RAGArchitecting Intelligence from Raw Data UnitsProject Status: Production-Ready | Data Integrity: 99.90% | Architecture: RAG-Enabledüìñ The Engineering MissionThis repository documents the end-to-end transformation of high-volume, unstructured industry data into a semantically-aware AI knowledge system. Processing a total of 13,952 data units extracted from UK industry reports, this project bridges the gap between raw information and actionable intelligence.üèóÔ∏è Pipeline Architecture at a GlanceThe system is built on a four-stage industrial pipeline:Validation: A rigorous "Gatekeeper" phase ensuring a 99.90% integrity rate across 139 JSON data chunks.Enrichment: A semantic taxonomy layer that categorizes data into Public Policy, Business Impact, and Future of Work sectors.Vectorization: Deployment of a local Sentence-Transformer model to map 13,938 validated units into a 384-dimensional neural space.Retrieval (RAG): A custom-built query engine that provides context-filtered, high-precision search results using Cosine Similarity.This project serves as a blueprint for local-first AI architectures, prioritizing data privacy, technical rigor, and semantic accuracy.üìå Project OverviewThis project demonstrates the end-to-end engineering required to transform raw data into a professional AI knowledge system. Whether working with a custom dataset or your own proprietary data, this repository provides the blueprint for two distinct integration paths:MCP (Model Context Protocol): Direct tool-based connection.RAG (Retrieval-Augmented Generation): Semantic vector-based memory.The objective is to guide the transition from raw, fragmented files to a production-ready AI context.<p align="center"> <h2>üõ† THE 5-STAGE ENGINEERING PIPELINE</h2> </p><details><summary><b>Click to expand the 5-Stage Curriculum details</b></summary>Stage 1: Data Validation & Schema Enforcement: Implementing scripts to ensure every unit adheres to a strict schema, removing duplicates and correcting encoding errors.Stage 2: Semantic Metadata Enrichment: Programmatically tagging chunks with relevant metadata (Zone A, B, or C).Stage 3: Vectorization & Neural Embeddings: Converting text into high-dimensional vectors for semantic search.Stage 4: Vector Database Implementation: Utilizing local storage for sub-second retrieval.Stage 5: Inference & Pipeline Integration: Connecting the local engine to the data for grounded AI answers.</details><p align="center"> <h2>üè∑Ô∏è STAGE 2: SEMANTIC TAXONOMY</h2> </p>To transform raw data into a structured knowledge base, we categorize each unit into one of three thematic "Zones":ZoneCategoryLogic / KeywordsGoalZone APublic & PolicyGovernment, NHS, Ethics, SafetyRegulatory guardrails.Zone BBusiness ImpactInvestment, Market, Revenue, StartupsEconomic growth.Zone CFuture of WorkSkills, Automation, Jobs, TrainingLabor evolution.üìä Latest Audit StatisticsTotal Individual Data Units: 13,952Successfully Validated: 13,938Integrity Rate: 99.90%<p align="center"> <h2>üß† STAGE 3: NEURAL VECTORIZATION</h2> </p>To enable semantic search, the enriched dataset was transformed into dense vector embeddings. This allows the system to retrieve information based on meaning rather than just keyword matching.üî¨ Technical SpecificationsAttributeSpecificationModelall-MiniLM-L6-v2 (Sentence-Transformers)Dimensionality384-dimensional dense vectorsTotal Vectors13,938FormatCompressed NumPy Archive (.npz)<p align="center"> <h2>üöÄ STAGE 4: FINAL VALIDATION & RESULTS</h2> </p>The pipeline successfully processed 13,952 total units with a 99.90% integrity rate.Example Query Success:When asked about 'Government safety standards for AI', the RAG engine successfully bypassed technical code noise to retrieve high-relevance policymaking evidence from Zone A, citing responsibilities from international bodies like the OECD.üõ†Ô∏è Execution RoadmapTo reproduce the results of this project, execute the scripts in the following sequence within your virtual environment:1Ô∏è‚É£ Data ValidationBashpython validate_chunks.py
Ensures the 13,952 units meet the 99.90% integrity threshold.2Ô∏è‚É£ Semantic EnrichmentBashpython enrich_metadata.py
Applies the Zone A/B/C taxonomy to the validated JSON units.3Ô∏è‚É£ Neural VectorizationBashpython create_embeddings.py
Generates the 384-dimensional mathematical coordinates for semantic search.4Ô∏è‚É£ RAG QueryingBashpython query_engine.py
Launches the interactive terminal to ask questions and retrieve context-aware answers.üìÇ Repository AssetsCore Scripts: validate_chunks.py, enrich_metadata.py, create_embeddings.py, query_engine.pyData Reports: stage_1_validation_report.mdNeural Store: semantic_vectors.npz
